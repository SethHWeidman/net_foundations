{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output After Training:\n",
      "[[ 7.46187116]\n",
      " [-0.37718946]\n",
      " [-3.36186709]]\n",
      "[[ 0.03352639]\n",
      " [ 0.02323634]\n",
      " [ 0.9836892 ]\n",
      " [ 0.97639195]\n",
      " [ 0.9836892 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# sigmoid function\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# input dataset\n",
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1],\n",
    "                [1,0,1]])\n",
    "    \n",
    "# output dataset            \n",
    "y = np.array([[0,0,1,1,1]]).T\n",
    "\n",
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "W = np.random.randn(3, 1)\n",
    "\n",
    "for iter in xrange(1000):\n",
    "\n",
    "    # forward propagation\n",
    "    A = np.dot(X,W)\n",
    "    H = sigmoid(A)\n",
    "    \n",
    "    # how much did we miss?\n",
    "    L = 0.5 * ( y - H ) ** 2\n",
    "    \n",
    "    dLdH = -1.0 * (y - H)\n",
    "    \n",
    "    dHdA = sigmoid(H, True)\n",
    "\n",
    "    # multiply how much we missed by the\n",
    "    # slope of the sigmoid at the values in l1\n",
    "    dLdA = dLdH * dHdA\n",
    "    \n",
    "    # xij = observation i, feature j\n",
    "    # A1 = w1 * x11 + w2 * x12 + w3 * x13\n",
    "    # A2 = w1 * x21 + w2 * x22 + w3 * x23\n",
    "    # A3 = w1 * x31 + w2 * x32 + w3 * x33\n",
    "    \n",
    "    dAdW = X.T\n",
    "    \n",
    "    # 3 x 1 = 3 x n * n x 1\n",
    "    dLdW = np.dot(dAdW, dLdA)\n",
    "    # Weight i, observation j is updated by sum(j=1^n) (Xi * dLdA),\n",
    "    # where j runs over all the observations\n",
    "    # times the product of the other derivatives,\n",
    "    # as we expect \n",
    "    \n",
    "    # dA / dW = [[x11 x12 x13]\n",
    "    #            [x21 x22 x23]\n",
    "    #            ...\n",
    "    #            [xn1 xn2 xn3]]\n",
    "    # dLdA = [A1 A2 ... An]\n",
    "    \n",
    "    # update weights\n",
    "    # w1 -= A1 * x11 + A2 * x21 + ... + An * xn1\n",
    "    # w2 -= A1 * x12 + A2 * x22 + ... + An * xn2\n",
    "    # w3 -= A1 * x13 + A2 * x23 + ... + An * xn3\n",
    "    W -= dLdW\n",
    "\n",
    "print \"Output After Training:\"\n",
    "print W\n",
    "print H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.126916753852\n",
      "Error:1.92407935252e-05\n",
      "Error:9.46304363495e-06\n",
      "Error:6.26225354584e-06\n",
      "Error:4.67565755863e-06\n",
      "Error:3.72887254869e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def nonlin(x,deriv=False):\n",
    "\tif(deriv==True):\n",
    "\t    return x*(1-x)\n",
    "\treturn 1/(1+np.exp(-x))\n",
    "    \n",
    "X = np.array([[0,0,1],\n",
    "            [0,1,1],\n",
    "            [1,0,1],\n",
    "            [1,1,1],\n",
    "             [1,1,0],\n",
    "             [0,0,0],\n",
    "             [0,1,0]])\n",
    "                \n",
    "y = np.array([[0],\n",
    "              [0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "              [1],\n",
    "             [1]])\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "V = np.random.randn(3, 4)\n",
    "W = np.random.randn(4, 1)\n",
    "\n",
    "for j in xrange(60000):\n",
    "\n",
    "\t# Feed forward through layers 0, 1, and 2\n",
    "    A1 = np.dot(X,V)\n",
    "    A2 = nonlin(A1)\n",
    "    B1 = np.dot(A2,W)\n",
    "    B2 = nonlin(B1)\n",
    "\n",
    "    # how much did we miss the target value?\n",
    "    L = 0.5 * (y - B2) ** 2\n",
    "    \n",
    "    if ( j % 10000) == 0:\n",
    "        print \"Error:\" + str(np.mean(np.abs(L)))\n",
    "        \n",
    "    # in what direction is the target value?\n",
    "    # were we really sure? if so, don't change too much.\n",
    "    dLdB2 = -1.0 * (y-B2)\n",
    "    \n",
    "    dB2dB1 = nonlin(B2,deriv=True)\n",
    "\n",
    "    dLdB1 = dLdB2*dB2dB1\n",
    "\n",
    "    dB1dW = A2.T\n",
    "    \n",
    "    dLdW = dB1dW.dot(dLdB1)\n",
    "    \n",
    "    dB1dA2 = W.T\n",
    "    \n",
    "    dLdA2 = dLdB1.dot(dB1dA2)\n",
    "    \n",
    "    dA2dA1 = nonlin(A2,deriv=True)\n",
    "    \n",
    "    dLdA1 = dLdA2 * dA2dA1\n",
    "    \n",
    "    dA1dV = X.T\n",
    "    \n",
    "    dLdV = dA1dV.dot(dLdA1)\n",
    "    \n",
    "    # W (4 x 1) = A2.T (4 x n) * B2_delta (n x 1)\n",
    "    W -= dLdW\n",
    "    \n",
    "    # V (3 x 4) = X.T (3 x n) * A1_delta (n x 4)\n",
    "    V -= dLdV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "toc": {
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
